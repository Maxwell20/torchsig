{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d579aba-7439-42a6-aae4-cf1983095dee",
   "metadata": {},
   "source": [
    "# Example 08 - Optuna with Wideband and YOLO\n",
    "This notebook showcases Optuna hyperparameter tuning a YOLOv8 model with Wideband.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a026bd-f096-47f3-a262-48ab5defe23e",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4bff6d5-4b2d-4db2-97a0-f45843e7cc60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Packages for Optuna\n",
    "from torchsig.utils.optuna.tuner import YoloOptunaOptimizer\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab949825-7c48-44f2-852d-56567f5952e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages Imports for Training\n",
    "from ultralytics import YOLO\n",
    "from torchsig.utils.yolo_train import Yolo_Trainer\n",
    "from torchsig.datasets.datamodules import WidebandDataModule\n",
    "from torchsig.transforms.transforms import Compose, Spectrogram, Normalize, SpectrogramImage\n",
    "from torchsig.transforms.target_transforms import DescToBBoxFamilyDict\n",
    "from torchsig.datasets.signal_classes import torchsig_signals\n",
    "import torch\n",
    "import yaml\n",
    "import numpy as np\n",
    "from torchsig.image_datasets.plotting.plotting import plot_yolo_datum\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efea64b7-1d22-41bd-81cc-5c39a74c3bdd",
   "metadata": {},
   "source": [
    "-----------------------------\n",
    "## Check or Generate the Wideband Dataset\n",
    "To generate the Wideband dataset, several parameters are given to the imported `WidebandDataModule` class. These paramters are:\n",
    "- `root` ~ A string to specify the root directory of where to generate and/or read an existing Wideband dataset\n",
    "- `train` ~ A boolean to specify if the Wideband dataset should be the training (True) or validation (False) sets\n",
    "- `qa` - A boolean to specify whether to generate a small subset of Wideband (True), or the full dataset (False), default is True\n",
    "- `impaired` ~ A boolean to specify if the Wideband dataset should be the clean version or the impaired version\n",
    "- `transform` ~ Optionally, pass in any data transforms here if the dataset will be used in an ML training pipeline. Note: these transforms are not called during the dataset generation. The static saved dataset will always be in IQ format. The transform is only called when retrieving data examples.\n",
    "- `target_transform` ~ Optionally, pass in any target transforms here if the dataset will be used in an ML training pipeline. Note: these target transforms are not called during the dataset generation. The static saved dataset will always be saved as tuples in the LMDB dataset. The target transform is only called when retrieving data examples.\n",
    "\n",
    "A combination of the `train` and the `impaired` booleans determines which of the four (4) distinct Wideband datasets will be instantiated:\n",
    "| `impaired` | `qa` | Result |\n",
    "| ---------- | ---- | ------- |\n",
    "| `False` | `False` | Clean datasets of train=250k examples and val=25k examples |\n",
    "| `False` | `True` | Clean datasets of train=250 examples and val=250 examples |\n",
    "| `True` | `False` | Impaired datasets of train=250k examples and val=25k examples |\n",
    "| `True` | `True` | Impaired datasets of train=250 examples and val=250 examples |\n",
    "\n",
    "The final option of the impaired validation set is the dataset to be used when reporting any results with the official Wideband dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca6e8bfb-92f3-4615-afac-7568350a2461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using WidebandImpairedTrainQAConfig for train.\n",
      "Dataset already exists in ./datasets/wideband/wideband_impaired_train. Not regenerating\n",
      "Using WidebandImpairedValQAConfig for val.\n",
      "Dataset already exists in ./datasets/wideband/wideband_impaired_val. Not regenerating\n",
      "Dataset length: 250\n",
      "Data shape: (262144,)\n",
      "Label: {'labels': tensor([5, 5, 5, 9]), 'boxes': tensor([[0.0950, 0.1786, 0.1900, 0.0491],\n",
      "        [0.5282, 0.5223, 0.1900, 0.0491],\n",
      "        [0.9331, 0.2768, 0.1337, 0.0491],\n",
      "        [0.5000, 0.5031, 1.0000, 0.1219]])}\n"
     ]
    }
   ],
   "source": [
    "# Generate Wideband DataModule\n",
    "root = \"./datasets/wideband\"\n",
    "impaired = True\n",
    "qa = True\n",
    "fft_size = 512\n",
    "class_list = torchsig_signals.class_list\n",
    "num_classes = len(class_list)\n",
    "batch_size = 1\n",
    "\n",
    "transform = Compose([    \n",
    "])\n",
    "\n",
    "target_transform = Compose([\n",
    "    DescToBBoxFamilyDict()\n",
    "])\n",
    "\n",
    "datamodule = WidebandDataModule(\n",
    "    root=root,\n",
    "    impaired=impaired,\n",
    "    qa=qa,\n",
    "    fft_size=fft_size,\n",
    "    num_classes=num_classes,\n",
    "    transform=transform,\n",
    "    target_transform=target_transform,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "datamodule.prepare_data()\n",
    "datamodule.setup(\"fit\")\n",
    "\n",
    "wideband_train = datamodule.train\n",
    "\n",
    "# Retrieve a sample and print out information\n",
    "idx = np.random.randint(len(wideband_train))\n",
    "data, label = wideband_train[idx]\n",
    "print(\"Dataset length: {}\".format(len(wideband_train)))\n",
    "print(\"Data shape: {}\".format(data.shape))\n",
    "print(\"Label: {}\".format(label))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2938efda-7b7f-42e9-8bcf-022ebcaf2d32",
   "metadata": {},
   "source": [
    "## Prepare YOLO trainer and Model\n",
    "Next, the datasets are rewritten to disk that is Ultralytics YOLO compatible. See [Ultralytics: Train Custom Data - Organize Directories](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data/#23-organize-directories) to learn more. \n",
    "\n",
    "Additionally, create a yaml file for dataset configuration. See [Ultralytics: Train Custom Data - Create dataset.yaml](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data/#21-create-datasetyaml)\n",
    "\n",
    "Download desired YOLO model from [Ultralytics Models](https://docs.ultralytics.com/models/). We will use YOLOv8, specifically `yolov8n.pt`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f5ad86-bb23-4396-8cd3-3b1c1c9a32b2",
   "metadata": {},
   "source": [
    "### Explanation of the `overrides` Dictionary\n",
    "\n",
    "The `overrides` dictionary is used to customize the settings for the Ultralytics YOLO trainer by specifying specific values that override the default configurations. The dictionary is imported from `wbdata.yaml`. However, you can customize in the notebook. \n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "overrides = {'model': 'yolov8n.pt', 'epochs': 100, 'data': '08_example.yaml', 'device': 0, 'imgsz': 512, 'single_cls': True}\n",
    "```\n",
    "A .yaml is necessary for training. Look at `08_yolo_optuna.yaml` in the examples directory. It will contain the path to your torchsig data.\n",
    "\n",
    "\n",
    "### Dataset Location Warning\n",
    "\n",
    "There must exist a datasets directory at `/path/to/torchsig/datasets`.\n",
    "\n",
    "This example assumes that you have generated `train` and `val` lmdb wideband datasets at `./datasets/wideband/`\n",
    "\n",
    "You can also specify an absolute path to your dataset in `08_yolo_optuna.yaml`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc6137ed-44f5-4dd3-a16c-0267a84bfe55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating experiment -> 08_example\n"
     ]
    }
   ],
   "source": [
    "# define dataset variables for yaml file\n",
    "config_name = \"08_yolo_optuna.yaml\"\n",
    "classes = {v: k for v, k in enumerate(class_list)}\n",
    "yolo_root = \"./wideband/\" # train/val images (relative to './datasets``\n",
    "\n",
    "# define overrides\n",
    "overrides = dict(\n",
    "    model = \"yolov8n.pt\",\n",
    "    project = \"yolo\",\n",
    "    name = \"08_example\",\n",
    "    epochs = 10,\n",
    "    imgsz = 512,\n",
    "    data = config_name,\n",
    "    device = 0 if torch.cuda.is_available() else \"cpu\",\n",
    "    single_cls = True,\n",
    "    batch = 32,\n",
    "    workers = 8\n",
    "\n",
    ")\n",
    "\n",
    "# create yaml file for trainer\n",
    "yolo_config = dict(\n",
    "    overrides = overrides,\n",
    "    train = yolo_root,\n",
    "    val = yolo_root,\n",
    "    nc = num_classes,\n",
    "    names = classes\n",
    ")\n",
    "\n",
    "with open(config_name, 'w+') as file:\n",
    "    yaml.dump(yolo_config, file, default_flow_style=False)\n",
    "print(f\"Creating experiment -> {overrides['name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7ccf16",
   "metadata": {},
   "source": [
    "## Run Optuna\n",
    "Now we can start using optuna to tune the following hyperparameters in YOLO:\n",
    "- `lr0` - Initial learning rate.\n",
    "- `cos_lr` - Toggle cosine learning rate scheduler.\n",
    "- `optimizer` - Choose optimizer (SGD, Adam, AdamW)\n",
    "- `freeze` - Freeze the first N layers of the model.\n",
    "- `imgsz` - Target image size for training.\n",
    "\n",
    "Within the optuna optimizer,`n_trials` determines how many trials to run, while `epochs` is how many epochs are run per trial.\n",
    "\n",
    "See [Ultralytics Train Settings](https://docs.ultralytics.com/usage/cfg/#train-settings) for more hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d7017ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-14 13:41:59,595] A new study created in memory with name: Test\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.3 ðŸš€ Python-3.10.12 torch-2.4.1+cu121 CUDA:0 (NVIDIA GeForce RTX 4090 Laptop GPU, 15981MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=08_yolo_optuna.yaml, epochs=1, time=None, patience=100, batch=32, imgsz=256, save=False, save_period=-1, cache=False, device=0, workers=1, project=yolo, name=08_example4, exist_ok=False, pretrained=True, optimizer=SGD, verbose=False, seed=0, deterministic=True, single_cls=True, rect=False, cos_lr=True, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=3, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=4, nms=False, lr0=0.0072616937413024565, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=yolo/08_example4\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir yolo/08_example4', view at http://localhost:6006/\n",
      "Overriding model.yaml nc=80 with nc=62\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    442762  ultralytics.nn.modules.head.Detect           [62, [64, 128, 256]]          \n",
      "Model summary: 249 layers, 2,702,298 parameters, 2,702,282 gradients, 7.0 GFLOPs\n",
      "\n",
      "Transferred 313/391 items from pretrained weights\n",
      "Freezing layer 'model.0.conv.weight'\n",
      "Freezing layer 'model.0.bn.weight'\n",
      "Freezing layer 'model.0.bn.bias'\n",
      "Freezing layer 'model.1.conv.weight'\n",
      "Freezing layer 'model.1.bn.weight'\n",
      "Freezing layer 'model.1.bn.bias'\n",
      "Freezing layer 'model.2.cv1.conv.weight'\n",
      "Freezing layer 'model.2.cv1.bn.weight'\n",
      "Freezing layer 'model.2.cv1.bn.bias'\n",
      "Freezing layer 'model.2.cv2.conv.weight'\n",
      "Freezing layer 'model.2.cv2.bn.weight'\n",
      "Freezing layer 'model.2.cv2.bn.bias'\n",
      "Freezing layer 'model.2.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.2.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.2.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.2.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.2.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.2.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLO11n...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n",
      "Loading label caches from /home/mtwente/torchsig/examples/datasets/wideband\n",
      "Loading label caches from /home/mtwente/torchsig/examples/datasets/wideband\n",
      "Plotting labels to yolo/08_example4/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.0072616937413024565, momentum=0.937) with parameter groups 63 weight(decay=0.0), 70 weight(decay=0.0005), 69 bias(decay=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/01/14 13:42:03 WARNING mlflow.utils.autologging_utils: You are using an unsupported version of pytorch. If you encounter errors during autologging, try upgrading / downgrading pytorch to a supported version, or try upgrading MLflow.\n",
      "2025/01/14 13:42:03 INFO mlflow.tracking.fluent: Autologging successfully enabled for pytorch_lightning.\n",
      "2025/01/14 13:42:03 INFO mlflow.tracking.fluent: Autologging successfully enabled for keras.\n",
      "2025/01/14 13:42:03 INFO mlflow.tracking.fluent: Autologging successfully enabled for tensorflow.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mMLflow: \u001b[0mlogging run_id(a33cc226e56745d08af344e88e9689fd) to runs/mlflow\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mview at http://127.0.0.1:5000 with 'mlflow server --backend-store-uri runs/mlflow'\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mdisable with 'yolo settings mlflow=False'\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added âœ…\n",
      "Image sizes 256 train, 256 val\n",
      "Using 1 dataloader workers\n",
      "Logging results to \u001b[1myolo/08_example4\u001b[0m\n",
      "Starting training for 1 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/1     0.648G      4.817      6.063      2.681        251        256: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:12<00:00,  1.53s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00,  6.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        250       1116          0          0          0          0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1 epochs completed in 0.005 hours.\n",
      "Optimizer stripped from yolo/08_example4/weights/last.pt, 5.6MB\n",
      "Optimizer stripped from yolo/08_example4/weights/best.pt, 5.6MB\n",
      "\n",
      "Validating yolo/08_example4/weights/best.pt...\n",
      "Ultralytics 8.3.3 ðŸš€ Python-3.10.12 torch-2.4.1+cu121 CUDA:0 (NVIDIA GeForce RTX 4090 Laptop GPU, 15981MiB)\n",
      "Model summary (fused): 186 layers, 2,696,458 parameters, 0 gradients, 6.9 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 12.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        250       1116          0          0          0          0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speed: 0.1ms preprocess, 0.4ms inference, 0.0ms loss, 0.0ms postprocess per image\n",
      "Results saved to \u001b[1myolo/08_example4\u001b[0m\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mresults logged to runs/mlflow\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mdisable with 'yolo settings mlflow=False'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-14 13:42:22,682] Trial 0 finished with value: 0.0 and parameters: {'lr': 0.0072616937413024565, 'cos_lr': True, 'freeze': 3, 'imgsz_power2': 8, 'optimizer': 'SGD'}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.3 ðŸš€ Python-3.10.12 torch-2.4.1+cu121 CUDA:0 (NVIDIA GeForce RTX 4090 Laptop GPU, 15981MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=08_yolo_optuna.yaml, epochs=1, time=None, patience=100, batch=32, imgsz=64, save=False, save_period=-1, cache=False, device=0, workers=1, project=yolo, name=08_example5, exist_ok=False, pretrained=True, optimizer=AdamW, verbose=False, seed=0, deterministic=True, single_cls=True, rect=False, cos_lr=True, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=0, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=4, nms=False, lr0=0.0029050204520835476, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=yolo/08_example5\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir yolo/08_example5', view at http://localhost:6006/\n",
      "Overriding model.yaml nc=80 with nc=62\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    442762  ultralytics.nn.modules.head.Detect           [62, [64, 128, 256]]          \n",
      "Model summary: 249 layers, 2,702,298 parameters, 2,702,282 gradients, 7.0 GFLOPs\n",
      "\n",
      "Transferred 313/391 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLO11n...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n",
      "Loading label caches from /home/mtwente/torchsig/examples/datasets/wideband\n",
      "Loading label caches from /home/mtwente/torchsig/examples/datasets/wideband\n",
      "Plotting labels to yolo/08_example5/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.0029050204520835476, momentum=0.937) with parameter groups 63 weight(decay=0.0), 70 weight(decay=0.0005), 69 bias(decay=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/01/14 13:42:24 WARNING mlflow.utils.autologging_utils: You are using an unsupported version of pytorch. If you encounter errors during autologging, try upgrading / downgrading pytorch to a supported version, or try upgrading MLflow.\n",
      "2025/01/14 13:42:24 INFO mlflow.tracking.fluent: Autologging successfully enabled for pytorch_lightning.\n",
      "2025/01/14 13:42:24 INFO mlflow.tracking.fluent: Autologging successfully enabled for keras.\n",
      "2025/01/14 13:42:24 INFO mlflow.tracking.fluent: Autologging successfully enabled for tensorflow.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mMLflow: \u001b[0mlogging run_id(aadf85eb09b14545926eb13ca0d5aab5) to runs/mlflow\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mview at http://127.0.0.1:5000 with 'mlflow server --backend-store-uri runs/mlflow'\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mdisable with 'yolo settings mlflow=False'\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added âœ…\n",
      "Image sizes 64 train, 64 val\n",
      "Using 1 dataloader workers\n",
      "Logging results to \u001b[1myolo/08_example5\u001b[0m\n",
      "Starting training for 1 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/1     0.226G          0      2.424          0          0         64: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:05<00:00,  1.39it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 10.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        250       1116          0          0          0          0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1 epochs completed in 0.005 hours.\n",
      "Optimizer stripped from yolo/08_example5/weights/last.pt, 5.6MB\n",
      "Optimizer stripped from yolo/08_example5/weights/best.pt, 5.6MB\n",
      "\n",
      "Validating yolo/08_example5/weights/best.pt...\n",
      "Ultralytics 8.3.3 ðŸš€ Python-3.10.12 torch-2.4.1+cu121 CUDA:0 (NVIDIA GeForce RTX 4090 Laptop GPU, 15981MiB)\n",
      "Model summary (fused): 186 layers, 2,696,458 parameters, 0 gradients, 6.9 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 14.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        250       1116          0          0          0          0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speed: 0.0ms preprocess, 0.4ms inference, 0.0ms loss, 0.2ms postprocess per image\n",
      "Results saved to \u001b[1myolo/08_example5\u001b[0m\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mresults logged to runs/mlflow\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mdisable with 'yolo settings mlflow=False'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-14 13:42:46,319] Trial 1 finished with value: 0.0 and parameters: {'lr': 0.0029050204520835476, 'cos_lr': True, 'freeze': 0, 'imgsz_power2': 6, 'optimizer': 'AdamW'}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Number of finished trials: 2\n",
      "Best trial:\n",
      "  Value: 0.0\n",
      "  Params: \n",
      "    lr: 0.0072616937413024565\n",
      "    cos_lr: True\n",
      "    freeze: 3\n",
      "    imgsz_power2: 8\n",
      "    optimizer: SGD\n"
     ]
    }
   ],
   "source": [
    "opt = YoloOptunaOptimizer(overrides, n_trials=2, epochs=1)\n",
    "study, best_params = opt.run_optimization()\n",
    "\n",
    "overrides_optimized = opt.get_optimized_overrides()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22fa62fc-01d9-46b7-9cd2-38643f54d1de",
   "metadata": {},
   "source": [
    "## Train YOLO with Optimized Hyperparameters\n",
    "Train YOLO. See [Ultralytics Train](https://docs.ultralytics.com/modes/train/#train-settings) for training hyperparameter options.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad7e5f57-3b5b-4074-b87c-6e18c7b973af",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.3 ðŸš€ Python-3.10.12 torch-2.4.1+cu121 CUDA:0 (NVIDIA GeForce RTX 4090 Laptop GPU, 15981MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=08_yolo_optuna.yaml, epochs=10, time=None, patience=100, batch=32, imgsz=256, save=True, save_period=-1, cache=False, device=0, workers=8, project=yolo, name=08_example6, exist_ok=False, pretrained=True, optimizer=SGD, verbose=True, seed=0, deterministic=True, single_cls=True, rect=False, cos_lr=True, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=3, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=4, nms=False, lr0=0.0072616937413024565, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=yolo/08_example6\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir yolo/08_example6', view at http://localhost:6006/\n",
      "Overriding model.yaml nc=80 with nc=62\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    442762  ultralytics.nn.modules.head.Detect           [62, [64, 128, 256]]          \n",
      "Model summary: 249 layers, 2,702,298 parameters, 2,702,282 gradients, 7.0 GFLOPs\n",
      "\n",
      "Transferred 313/391 items from pretrained weights\n",
      "Freezing layer 'model.0.conv.weight'\n",
      "Freezing layer 'model.0.bn.weight'\n",
      "Freezing layer 'model.0.bn.bias'\n",
      "Freezing layer 'model.1.conv.weight'\n",
      "Freezing layer 'model.1.bn.weight'\n",
      "Freezing layer 'model.1.bn.bias'\n",
      "Freezing layer 'model.2.cv1.conv.weight'\n",
      "Freezing layer 'model.2.cv1.bn.weight'\n",
      "Freezing layer 'model.2.cv1.bn.bias'\n",
      "Freezing layer 'model.2.cv2.conv.weight'\n",
      "Freezing layer 'model.2.cv2.bn.weight'\n",
      "Freezing layer 'model.2.cv2.bn.bias'\n",
      "Freezing layer 'model.2.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.2.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.2.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.2.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.2.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.2.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLO11n...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n",
      "Loading label caches from /home/mtwente/torchsig/examples/datasets/wideband\n",
      "Loading label caches from /home/mtwente/torchsig/examples/datasets/wideband\n",
      "Plotting labels to yolo/08_example6/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.0072616937413024565, momentum=0.937) with parameter groups 63 weight(decay=0.0), 70 weight(decay=0.0005), 69 bias(decay=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/01/14 13:42:52 WARNING mlflow.utils.autologging_utils: You are using an unsupported version of pytorch. If you encounter errors during autologging, try upgrading / downgrading pytorch to a supported version, or try upgrading MLflow.\n",
      "2025/01/14 13:42:52 INFO mlflow.tracking.fluent: Autologging successfully enabled for pytorch_lightning.\n",
      "2025/01/14 13:42:52 INFO mlflow.tracking.fluent: Autologging successfully enabled for keras.\n",
      "2025/01/14 13:42:52 INFO mlflow.tracking.fluent: Autologging successfully enabled for tensorflow.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mMLflow: \u001b[0mlogging run_id(6c0611907a5a4e2eb95612da2a2927f0) to runs/mlflow\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mview at http://127.0.0.1:5000 with 'mlflow server --backend-store-uri runs/mlflow'\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mdisable with 'yolo settings mlflow=False'\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added âœ…\n",
      "Image sizes 256 train, 256 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1myolo/08_example6\u001b[0m\n",
      "Starting training for 10 epochs...\n",
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/10     0.656G      5.113      6.648      3.252         87        256: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:01<00:00,  7.60it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 15.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        250       1116          0          0          0          0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       2/10     0.617G      4.481       6.31      2.574         67        256: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 19.79it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 17.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        250       1116          0          0          0          0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       3/10     0.617G      3.357       5.86      1.784         76        256: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 20.14it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 19.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        250       1116          0          0          0          0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       4/10      0.64G      2.665      5.434      1.471         77        256: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 18.28it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 19.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        250       1116          0          0          0          0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       5/10      0.64G      2.487      5.131      1.418         71        256: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 18.68it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 20.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        250       1116          0          0          0          0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       6/10      0.64G      2.366      4.751      1.361         92        256: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 18.51it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 12.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        250       1116          0          0          0          0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       7/10     0.648G      2.284      4.366      1.356         80        256: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 20.01it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 18.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        250       1116          0          0          0          0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       8/10     0.646G      2.156      4.037      1.308         90        256: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 20.50it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 22.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        250       1116          0          0          0          0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       9/10     0.648G      2.219      3.766      1.325         87        256: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 24.07it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 18.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        250       1116          0          0          0          0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      10/10     0.648G      2.139      3.499      1.303         78        256: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 20.61it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 16.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        250       1116          1   0.000896        0.5       0.35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "10 epochs completed in 0.007 hours.\n",
      "Optimizer stripped from yolo/08_example6/weights/last.pt, 5.6MB\n",
      "Optimizer stripped from yolo/08_example6/weights/best.pt, 5.6MB\n",
      "\n",
      "Validating yolo/08_example6/weights/best.pt...\n",
      "Ultralytics 8.3.3 ðŸš€ Python-3.10.12 torch-2.4.1+cu121 CUDA:0 (NVIDIA GeForce RTX 4090 Laptop GPU, 15981MiB)\n",
      "Model summary (fused): 186 layers, 2,696,458 parameters, 0 gradients, 6.9 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 15.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        250       1116          1   0.000896        0.5       0.35\n",
      "                   ook        250       1116          1   0.000896        0.5       0.35\n",
      "Speed: 0.1ms preprocess, 0.2ms inference, 0.0ms loss, 0.0ms postprocess per image\n",
      "Results saved to \u001b[1myolo/08_example6\u001b[0m\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mresults logged to runs/mlflow\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mdisable with 'yolo settings mlflow=False'\n"
     ]
    }
   ],
   "source": [
    "trainer = Yolo_Trainer(overrides=overrides_optimized)\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad0c459-2367-4f7d-89ae-5f4cfd8f545f",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "Check model performance from training. From here, you can use the trained model to test on prepared data (numpy image arrays of spectrograms)\n",
    "\n",
    "Will load example from Torchsig\n",
    "\n",
    "model_path is path to best.pt from your training session. Path is printed at the end of training.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45fa7bf-fa7d-4896-9022-4e608d93e5a4",
   "metadata": {},
   "source": [
    "## Generate and Instantiate WBSig53 Test Dataset\n",
    "After generating the WBSig53 dataset (see `03_example_Wideband_dataset.ipynb`), we can instantiate it with the needed transforms. Change `root` to test dataset path.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "694c33f1-d718-40ae-861d-26c3431a6f41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using WidebandImpairedTrainQAConfig for train.\n",
      "Dataset already exists in ./datasets/wideband_test/wideband_impaired_train. Not regenerating\n",
      "Using WidebandImpairedValQAConfig for val.\n",
      "Dataset already exists in ./datasets/wideband_test/wideband_impaired_val. Not regenerating\n",
      "Dataset length: 250\n",
      "Data shape: (512, 512, 3)\n"
     ]
    }
   ],
   "source": [
    "test_path = './datasets/wideband_test' #Should differ from your training dataset\n",
    "\n",
    "transform = Compose([\n",
    "    Spectrogram(nperseg=512, noverlap=0, nfft=512, mode='psd'),\n",
    "    Normalize(norm=np.inf, flatten=True),\n",
    "    SpectrogramImage(), \n",
    "    ])\n",
    "\n",
    "test_data = WidebandDataModule(\n",
    "    root=test_path,\n",
    "    impaired=impaired,\n",
    "    qa=qa,\n",
    "    fft_size=fft_size,\n",
    "    num_classes=num_classes,\n",
    "    transform=transform,\n",
    "    target_transform=None,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "test_data.prepare_data()\n",
    "test_data.setup(\"fit\")\n",
    "\n",
    "wideband_test = test_data.train\n",
    "\n",
    "# Retrieve a sample and print out information\n",
    "idx = np.random.randint(len(wideband_test))\n",
    "data, label = wideband_test[idx]\n",
    "print(\"Dataset length: {}\".format(len(wideband_test)))\n",
    "print(\"Data shape: {}\".format(data.shape))\n",
    "\n",
    "samples = []\n",
    "labels = []\n",
    "for i in range(10):\n",
    "    idx = np.random.randint(len(wideband_test))\n",
    "    sample, label = wideband_test[idx]\n",
    "    lb = [l['class_name'] for l in label]\n",
    "    samples.append(sample)\n",
    "    labels.append(lb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9947fc-9636-415c-af50-26d7e5d95953",
   "metadata": {},
   "source": [
    "### Load model \n",
    "The model path is printed after training. Use the best.pt weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76e73a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2bb8735-ce6c-482b-9675-302f6848b0ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 512x512 (no detections), 3.4ms\n",
      "1: 512x512 (no detections), 3.4ms\n",
      "2: 512x512 (no detections), 3.4ms\n",
      "3: 512x512 (no detections), 3.4ms\n",
      "4: 512x512 (no detections), 3.4ms\n",
      "5: 512x512 (no detections), 3.4ms\n",
      "6: 512x512 (no detections), 3.4ms\n",
      "7: 512x512 (no detections), 3.4ms\n",
      "8: 512x512 (no detections), 3.4ms\n",
      "9: 512x512 (no detections), 3.4ms\n",
      "Speed: 1.0ms preprocess, 3.4ms inference, 0.1ms postprocess per image at shape (1, 3, 512, 512)\n",
      "Results saved to \u001b[1mruns/detect/predict4\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "model = YOLO(trainer.best)\n",
    "# Inference will be saved to path printed after predict. \n",
    "results = model.predict(samples, save=True, imgsz=512, conf=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "107d38d4-36af-4d0b-8bd9-926228b3e4ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual Labels -> ['16msk', 'ofdm-600', 'ofdm-256']\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'img'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m probs \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mprobs  \u001b[38;5;66;03m# Probs object for classification outputs\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mActual Labels -> \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabels[y]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m \u001b[43mplot_yolo_datum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresults\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# result.show()  # display to screen\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torchsig/image_datasets/plotting/plotting.py:19\u001b[0m, in \u001b[0;36mplot_yolo_datum\u001b[0;34m(yolo_datum)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot_yolo_datum\u001b[39m(yolo_datum):\n\u001b[0;32m---> 19\u001b[0m     plot_yolo_boxes_on_image(\u001b[43myolo_datum\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimg\u001b[49m, yolo_datum\u001b[38;5;241m.\u001b[39mlabels)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'img'"
     ]
    }
   ],
   "source": [
    "# Process results list\n",
    "for y, result in enumerate(results):\n",
    "    boxes = result.boxes  # Boxes object for bounding box outputs\n",
    "    probs = result.probs  # Probs object for classification outputs\n",
    "    print(f'Actual Labels -> {labels[y]}')\n",
    "    plot_yolo_datum(results)\n",
    "    # result.show()  # display to screen"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
